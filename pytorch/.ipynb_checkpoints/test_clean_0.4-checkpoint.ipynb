{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST fully connected two hidden layers.\n",
    "# Proper implementation of Fisher info estimation.\n",
    "# 0.4 = adapted for pytorch 0.4\n",
    "\n",
    "# Reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Inline plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Data loading\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Network\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Standard\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Local files\n",
    "from read_data_permuted import read_data_permuted # Read datasets w/ transformations\n",
    "from utility import permutate, trans # Data preprocessing\n",
    "from utility import calc_EWC_loss, calc_L2_loss # Extra loss functions\n",
    "from utility import calc_Fisher # Calculating Fisher Info.\n",
    "from utility import test_acc # Calculate test accuracy\n",
    "from utility import plot_accuracy # Plot accuracy curves\n",
    "\n",
    "# Parameters\n",
    "N_task = 3                  # Number of tasks\n",
    "N_epoch = 1               # Number of epochs\n",
    "batch_size = 4            # Number of samples in each minibatch\n",
    "hidden_size = 50           # Number of hidden layer neurons\n",
    "hidden1_dropout_prob = 0  # First hidden layer dropout probability\n",
    "hidden2_dropout_prob = 0  # Second hidden layer dropout probability\n",
    "lambda_L2 = 0.025           # Regularization parameter for L2\n",
    "lambda_EWC = 15              # Regularization parameter for EWC\n",
    "sample_size_Fish = 200    # Number of samples to use to estimate Fisher\n",
    "\n",
    "# Debugging options\n",
    "bool_debug = False # If true, don't loop through all data but stop after max_iter\n",
    "max_iter = 10\n",
    "\n",
    "# Miscellaneous options\n",
    "use_cuda = 1\n",
    "num_rec = 2000              # How often to record/display test accuracies, running loss, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read datasets and show samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST-specific settings\n",
    "# Height and width of images\n",
    "height = 28\n",
    "width = 28\n",
    "# Number of labels\n",
    "num_labels = 10\n",
    "# Labels (for display purposes)\n",
    "classes = ('0','1','2','3','4','5','6','7','8','9')\n",
    "\n",
    "trainsets, testsets = read_data_permuted(height*width,N_task) # Read datasets\n",
    "\n",
    "# Number of samples to display per task\n",
    "batch_size_sample = 4\n",
    "\n",
    "# Train dataset loaders\n",
    "trainloaders_sample = []\n",
    "for i in range(N_task):\n",
    "    trainloaders_sample.append(torch.utils.data.DataLoader(trainsets[i], batch_size=batch_size_sample, shuffle=True,num_workers=2))\n",
    "\n",
    "# Show image\n",
    "def imshow(img):\n",
    "    img = img/2 + 0.5   # Unnormalize ([-1,1] -> [0,1])\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    \n",
    "for i in range(N_task):\n",
    "    trainloader_sample = trainloaders_sample[i]\n",
    "    \n",
    "    dataiter = iter(trainloader_sample)     # Convert to iterator\n",
    "    images, labels = dataiter.next()        # Get next minibatch\n",
    "    \n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    plt.show()\n",
    "    \n",
    "    # Show labels\n",
    "    print(' '.join('%10s' % classes[labels[j]] for j in range(batch_size_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=400, hidden1_dropout_prob=0.2, hidden2_dropout_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        #self.fc1_drop = nn.Dropout(p=hidden1_dropout_prob)\n",
    "        #self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.fc2_drop = nn.Dropout(p=hidden2_dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.fc1_drop(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = self.fc2_drop(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Vanilla\n",
    "net = Net(\n",
    "    input_dim = height*width, \\\n",
    "    output_dim = num_labels, \\\n",
    "    hidden_size = hidden_size, \\\n",
    "    hidden1_dropout_prob = hidden1_dropout_prob, \\\n",
    "    hidden2_dropout_prob = hidden2_dropout_prob)\n",
    "\n",
    "net_L2 = copy.deepcopy(net)\n",
    "net_EWC = copy.deepcopy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer_L2 = optim.SGD(net_L2.parameters(), lr=0.01)\n",
    "optimizer_EWC = optim.SGD(net_EWC.parameters(),lr=0.01)\n",
    "\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#optimizer_L2 = optim.Adam(net_L2.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#optimizer_EWC = optim.Adam(net_EWC.parameters(),lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Record accuracy\n",
    "time_list = []\n",
    "\n",
    "# Each \"lists\" contains N_task lists that record train accuracy\n",
    "acc_lists_t = [[] for i in range(N_task)]\n",
    "acc_L2_lists_t = [[] for i in range(N_task)]\n",
    "acc_EWC_lists_t = [[] for i in range(N_task)]\n",
    "\n",
    "# Each \"lists\" contains N_task lists that record test accuracy\n",
    "acc_lists = [[] for i in range(N_task)]\n",
    "acc_L2_lists = [[] for i in range(N_task)]\n",
    "acc_EWC_lists = [[] for i in range(N_task)]\n",
    "\n",
    "# Record trained models after each task\n",
    "param_list_L2 = []\n",
    "param_list_EWC = []\n",
    "Fisher_list = []\n",
    "Fisher_sum = []\n",
    "\n",
    "# CUDA\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    net = net.cuda()\n",
    "    net_L2 = net_L2.cuda()\n",
    "    net_EWC = net_EWC.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "\n",
    "t_start = time.time()   # Record total runtime\n",
    "t_part1 = t_start        # Record partial runtime\n",
    "for task in range(len(trainsets)):\n",
    "    trainset = trainsets[task]\n",
    "    testset = testsets[task]\n",
    "    \n",
    "    for epoch in range(N_epoch):\n",
    "        # Reset running loss\n",
    "        running_loss = 0.0\n",
    "        running_loss_L2 = 0.0\n",
    "        running_loss_EWC = 0.0\n",
    "        \n",
    "        # Set up training data\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "        \n",
    "        # For debugging purposes\n",
    "        num_iter = 0\n",
    "\n",
    "        # Iterate through all train data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            num_iter += 1 # Debugging\n",
    "            \n",
    "            inputs, labels = data\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "            # Resize input as 1D array\n",
    "            inputs = inputs.view(-1,height*width)\n",
    "            \n",
    "            # Vanilla\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward\n",
    "            outputs = net(inputs)\n",
    "            # Backward\n",
    "            original_loss = criterion(outputs, labels)\n",
    "            loss = original_loss\n",
    "            loss.backward()\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            \n",
    "            # L2 regularization\n",
    "            # Zero the parameter gradients\n",
    "            optimizer_L2.zero_grad()\n",
    "            # Forward\n",
    "            outputs_L2 = net_L2(inputs)\n",
    "            # Backward\n",
    "            original_loss_L2 = criterion(outputs_L2, labels)\n",
    "            L2_loss = torch.zeros([],requires_grad=True)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                L2_loss = L2_loss.cuda()\n",
    "            # Single penalty\n",
    "            #if task > 0:\n",
    "            #    L2_loss = L2_loss + calc_L2_loss(net_L2, param_list_L2[task-1])\n",
    "            # Multiple penalties\n",
    "            for t_num in range(task):\n",
    "                L2_loss = L2_loss + calc_L2_loss(net_L2, param_list_L2[t_num])\n",
    "            \n",
    "            loss_L2 = original_loss_L2 + 0.5*lambda_L2*L2_loss\n",
    "            loss_L2.backward()\n",
    "            # Optimize\n",
    "            optimizer_L2.step()\n",
    "\n",
    "            # EWC\n",
    "            # Zero the parameter gradients\n",
    "            optimizer_EWC.zero_grad()\n",
    "            # Forward\n",
    "            outputs_EWC = net_EWC(inputs)\n",
    "            # Backward\n",
    "            original_loss_EWC = criterion(outputs_EWC, labels)\n",
    "            EWC_loss = torch.zeros([], requires_grad = True)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                EWC_loss = EWC_loss.cuda()\n",
    "            # Single penalty\n",
    "            #if task > 0:\n",
    "            #    EWC_loss = EWC_loss + calc_EWC_loss(net_EWC, param_list_EWC[task-1], Fisher_sum)\n",
    "            # Multiple penalties\n",
    "            for t_num in range(task):\n",
    "                EWC_loss = EWC_loss + calc_EWC_loss(net_EWC, param_list_EWC[t_num],Fisher_list[t_num])\n",
    "            \n",
    "            loss_EWC = original_loss_EWC + 0.5*lambda_EWC*EWC_loss\n",
    "            loss_EWC.backward()\n",
    "            # Optimize\n",
    "            optimizer_EWC.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            running_loss_L2 += loss_L2.data[0]\n",
    "            running_loss_EWC += loss_EWC.data[0]\n",
    "\n",
    "            if (i % num_rec) == (num_rec-1): # Print every num_rec mini-batches\n",
    "            \n",
    "                t_part2 = time.time()\n",
    "            \n",
    "                print('[%d, %5d] loss: %.3f (Vanilla) %.3f (L2) %.3f (EWC), time: %.0f s' % (epoch+1,i+1,running_loss/num_rec,running_loss_L2/num_rec,running_loss_EWC/num_rec,t_part2 - t_part1))\n",
    "                \n",
    "                t_part1 = t_part2\n",
    "                \n",
    "                # Reset running loss\n",
    "                running_loss = 0.0\n",
    "                running_loss_L2 = 0.0\n",
    "                running_loss_EWC = 0.0\n",
    "                \n",
    "                # Record test accuracies for each task\n",
    "                for j in range(N_task):\n",
    "                    acc_lists_t[j].append(test_acc(net, trainsets[j], height*width, batch_size, use_cuda))           # Vanilla\n",
    "                    acc_L2_lists_t[j].append(test_acc(net_L2, trainsets[j], height*width, batch_size, use_cuda))     # L2\n",
    "                    acc_EWC_lists_t[j].append(test_acc(net_EWC, trainsets[j], height*width, batch_size, use_cuda))   # EWC\n",
    "                    \n",
    "                    acc_lists[j].append(test_acc(net, testsets[j], height*width, batch_size, use_cuda))           # Vanilla\n",
    "                    acc_L2_lists[j].append(test_acc(net_L2, testsets[j], height*width, batch_size, use_cuda))     # L2\n",
    "                    acc_EWC_lists[j].append(test_acc(net_EWC, testsets[j], height*width, batch_size, use_cuda))   # EWC\n",
    "\n",
    "            if bool_debug and num_iter >= max_iter:\n",
    "                break;\n",
    "    \n",
    "    # Save models trained on the current task\n",
    "    current_param_L2 = copy.deepcopy(list(net_L2.parameters()))\n",
    "    param_list_L2.append(current_param_L2)\n",
    "    \n",
    "    current_param_EWC = copy.deepcopy(list(net_EWC.parameters()))\n",
    "    param_list_EWC.append(current_param_EWC)\n",
    "    \n",
    "    # Calculate Fisher Info. for EWC\n",
    "    \n",
    "    Fisher = calc_Fisher(net_EWC,testset,sample_size_Fish,use_cuda)\n",
    "    \n",
    "    # Record Fisher (for multiple penalties)\n",
    "    Fisher_list.append(Fisher)\n",
    "    # Sum Fisher (for single penalty)\n",
    "    if task == 0:\n",
    "        Fisher_sum = copy.deepcopy(Fisher)\n",
    "    else:\n",
    "        for i in range(len(Fisher)):\n",
    "            Fisher_sum[i] += Fisher[i]\n",
    "      \n",
    "t_finish = time.time()\n",
    "\n",
    "print('Finished Training')\n",
    "print('Training time: ' + str(t_finish-t_start) + ' s')\n",
    "\n",
    "# Convert to arrays\n",
    "for j in range(N_task):\n",
    "    acc_lists[j] = np.array(acc_lists[j])\n",
    "    acc_L2_lists[j] = np.array(acc_L2_lists[j])\n",
    "    acc_EWC_lists[j] = np.array(acc_EWC_lists[j])\n",
    "time_list = np.arange(len(acc_lists[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabelname = 'Train accuracy'\n",
    "\n",
    "plot_accuracy(time_list, acc_lists_t, acc_L2_lists_t, acc_EWC_lists_t, N_task, ylabelname, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabelname = 'Test accuracy'\n",
    "savename = 'noleg_new_multiple_pen_10_epochs_H=' + str(hidden_size) + '_lambda_L2=' + str(lambda_L2) + '_lambda_EWC=' + str(lambda_EWC)\n",
    "\n",
    "plot_accuracy(time_list, acc_lists, acc_L2_lists, acc_EWC_lists, N_task, ylabelname, True, savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot test averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = len(time_list)\n",
    "task_time = len(time_list)//N_task\n",
    "\n",
    "acc_avg = np.zeros(N_task)\n",
    "acc_L2_avg = np.zeros(N_task)\n",
    "acc_EWC_avg = np.zeros(N_task)\n",
    "\n",
    "for j in range(N_task):\n",
    "    for i in range(j+1):\n",
    "        acc_avg[j] += acc_lists[i][(j+1)*task_time-1]\n",
    "        acc_L2_avg[j] += acc_L2_lists[i][(j+1)*task_time-1]\n",
    "        acc_EWC_avg[j] += acc_EWC_lists[i][(j+1)*task_time-1]\n",
    "    acc_avg[j] /= (j+1)\n",
    "    acc_L2_avg[j] /= (j+1)\n",
    "    acc_EWC_avg[j] /= (j+1)\n",
    "    \n",
    "plt.figure()\n",
    "plt.title('Test accuracy (averaged over all learned tasks)')\n",
    "plt.plot(acc_avg, label = 'Vanilla',c='r')\n",
    "plt.plot(acc_L2_avg, label = 'L2',c='b')\n",
    "plt.plot(acc_EWC_avg, label = 'EWC',c='g')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Task')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('N_task=' + str(N_task) + 'new_multiple_pen_10_epochs_H=' + str(hidden_size) + '_lambda_L2=' + str(lambda_L2) + '_lambda_EWC=' + str(lambda_EWC) + '_avg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for p1,p2 in zip(net_L2.parameters(),blah):\n",
    "    losses.append(((p1-p2)**2).sum())\n",
    "losses\n",
    "sum(losses)\n",
    "\n",
    "calc_L2_loss(net_L2,blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True,num_workers=2)\n",
    "for a,b in dataloader:\n",
    "    a = a.view(1,-1)\n",
    "    a = Variable(a)\n",
    "    b = Variable(b)\n",
    "    a = a.cuda()\n",
    "    b = b.cuda()\n",
    "    net.eval()\n",
    "    #print(F.softmax(net(a)))\n",
    "    prob = F.softmax(net(a))\n",
    "    logL = F.log_softmax(net(a))[range(1),b.data]\n",
    "    net.train()\n",
    "    \n",
    "    #a = net(a).data.max(1)[1]\n",
    "    a=net(a)\n",
    "    print(a.data.max(1)[1][0])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#F.log_softmax(net(a))[range(1),b.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.log_softmax(net(a))[range(1),torch.multinomial(prob).data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fisher[5].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
